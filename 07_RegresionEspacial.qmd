---
format: 
    html:
        theme: cerulean
        toc: true
editor_options: 
  chunk_output_type: console
---

# Regresión espacial {#sec-SpatReg}

## Introducción. Regresión lineal y estructura espacial

En las unidades acerca de la regresión lineal hemos visto que uno de los supuestos que deben cumplir los datos es el de independencia de los residuos. Sin embargo, sabemos por la primera ley de la geografía que las observaciones más próximas suelen ser más similares entre sí. De hecho, en el momento en que nuestros datos presenten algún tipo de estructura espacial, se inclumplirá muy probablemente el supuesto de independencia, ya que los residuos del modelo fácilmente estarán espacialmente autocorrelacionados.

Cuando realizamos una regresión lineal solíamos evaluar las asunciones utilizando los residuos. Tomemos como ejemplo los datos de estaciones meteorológicas que hemos estado trabajando durante el curso. En este caso, como tendremos en cuenta la estructura espacial, usaremos el shapefile:

```{r, message = FALSE, warning=FALSE}
library(tmap)
library(sfdep)
library(spdep)
library(spatialreg)
library(spgwr)


estaciones <- st_read('data/meteo/meteo_espacial/estaciones_meteo.shp') 
provincias <- st_read('data/meteo/meteo_espacial/provincias_spain.shp') 


```

Vamos a ajustar un modelo de regresión lineal *normal*, no espacial, como hacíamos en la primera unidad del curso. En este caso, usaremos como variable dependiente `T_MAX_abs`, la temperatura máxima absoluta del mes de junio en cada estación, y como variable explicativa, la elevación:

```{r}

mod_lm <- lm(T_MAX_abs ~ ALTITUD, data = estaciones)
summary(mod_lm)
plot(mod_lm)
```

Vemos que el modelo parece adecuado: la variable predictora es significativa, el modelo no presenta problemas de falta de normalidad o heterocedasticidad, y además explica casi el 50% de la variabilidad en temperatura. Con lo visto hasta ahora, aceptaríamos el modelo sin ningún problema.

Sin embargo, si los datos de origen (las temperaturas) presentan autocorrelación espacial, es muy posible que los residuos del modelo también la presenten. Podemos comprobarlo, de momento visualmente. En primer lugar evaluaremos los valores de temperatura:

```{r}
tm_shape(estaciones) + 
    tm_dots(col = "T_MAX_abs", size = 1) +
    tm_legend(outside = TRUE, text.size = .8) +
    tm_shape(provincias) +
    tm_borders()
```

Es posible que haya cierta autocorrelación. Ahora veamos lo mismo para los residuos del modelo:

```{r}
# Guardamos los residuos dentro del dataset espacial
estaciones$residuals <- residuals(mod_lm)

tm_shape(estaciones) + 
    tm_dots( col = "residuals", palette="YlGn", size = 1) +
    tm_legend(outside = TRUE, text.size = .8) +
    tm_shape(provincias) +
    tm_borders()

```

También parece que puede haber cierta autocorrelación en los residuos. Vamos a comprobarlo de manera más cuantitativa. Recordemos, del tema de autocorrelación espacial, que primero debemos crear una lista de vecinos, y luego ya podemos calcular la autocorrelación. En este caso usaremos los 10 vecinos más próximos:

```{r}
veins <- st_knn(estaciones, k = 10)
```

Una vez definidos los vecinos podemos ahora testar la autocorrelación de los residuos del modelo de regresión. Para ello usaremos la función `lm.morantest()`, en vez de la típica que usábamos. Esta función tiene algunos detalles que la hacen más adecuada para testar la autocorrelación de residuos de un modelo, y tiene como argumentos un modelo de regresión lineal y los vecinos, que debemos transformar antes en un objeto de tipo "lista de vecinos" mediante la función `nb2listw()` :

```{r}
list_veins <- nb2listw(veins)

lm.morantest(mod_lm, list_veins)
```

Vemos que aunque el valor de la I de Moran es bastante bajo `r round(lm.morantest(mod_lm, nb2listw(veins))$estimate[1],4)`, sí que resulta significativo, lo que indica que existe una cierta autocorrelación en los residuos. En consecuencia, podríamos decir que el modelo no cumple el principio de independencia de las observaciones. Esto no implica necesariamente que debamos descartar el modelo, pero sí que deberíamos intentar tener en cuenta la estructura espacial de los datos en la regresión, para lo que podemos ajustar un modelo de regresión que tenga en cuenta la estructura espacial de los datos.

## Modelos de regresión espacial

El hecho de que exista autocorrelación espacial en los residuos puede deberse a varias razones. Puede que nuestra variable respuesta realmente tenga dinámicas de dependencia espacial, pero también puede ser que la relación entre los residuos se deba a la manera en que se agregan las unidades espaciales (por ejemplo, si medimos por municipios, o por rodales) o a que no hemos considerado una variable importante que sí varía espacialmente (por ejemplo, las características del suelo).

Existen en realidad dos tipos de modelos de regresión espacial, que difieren básicamente en cómo tratan la autocorrelación espacial de los residuos: un modelo de *error espacial* lo trata como si la agrupación espacial de los residuos fuera algo accidental, que se produce por azar o por variables que no hemos tenido en cuenta, y por tanto no asume que la variable *y* presente autocorrelación (pero sí los residuos).

Por otro lado, un modelo de *lag espacial* sí asume que los valores de *y* en una posición *i* dependerán de los valores de *y* en el vecindario de *i*. Estos modelos simplemente introducen la estructura espacial como una variable explicativa más en el modelo (lo veremos más adelante).

A menudo la realidad es una combinación de los dos procesos - accidente y causa subyacente -. Podemos ajustar ambos tipos de modelo y tomar la decisión según cuál se ajusta mejor *a posteriori*.

### Modelo de error espacial (Spatial error model)

Vamos a ajustar en primer lugar un modelo de error espacial, que asume que no hay causalidad en la autocorrelación de los residuos, es decir, que sólo los residuos están correlacionados, pero no la variable de interés.

![Fig.1. 3Estructura de un modelo de regresión espacial](images/spatial_error.png){fig-align="center"}

Si partimos de un modelo de regresión lineal convencional, que toma la forma:

$y_i= \beta_0 + \beta_1X_1+\epsilon_i$

el término $\epsilon_i$ puede partirse en dos partes, una que incluye la estructura espacial y otra que no:

$\epsilon_i = λω_i\epsilon_i+u_i$

por lo tanto:

$y_i= \beta_0 + \beta_1X_1+\lambda\omega_i\epsilon_i+u_i$

donde el término $\lambda\omega_i\epsilon_i$ representa la parte "espacial" del error, que se extrae del error general y se añade como una variable explicativa implícita. $\lambda$ es el coeficiente que multiplica a este error espacial, y si $\lambda=0$ entonces podemos concluir que no hay estructura espacial.

Para ajustar este modelo usaremos la función `errorsarlm()`, del paquete `spatialreg`. Debemos definir la fórmula de la regresión que queremos ajustar, el dataset (`estaciones`), y el listado de los vecinos (`list_veins`)

```{r}
error_model <- errorsarlm(formula =T_MAX_abs ~ ALTITUD, data = estaciones, listw = list_veins, zero.policy = T)

summary(error_model)
```

La salida de este tipo de modelo nos resultará familiar, muy similar a la de un modelo de regresión clásico. Vemos, por ejemplo, los estimadores, errores y significaciones de los coeficientes, igual que en la regresión lineal tradicional (si bien aquí no se añaden estrellas a los p-valores). El coeficiente de `ALTITUD` es ligeramente diferente que en el modelo tradicional.

Debajo de esta información sobre los coeficientes tenemos los valores de $\lambda$, que sería el parámetro que multiplica al término espacial del error que hemos creado, y un test LR (*likelihood ratio*) para determinar si ese término espacial del error es significativo. Además se compara el modelo ajustado con el modelo de regresión lineal original via el criterio de Información de Akaike (AIC). Finalmente, se utiliza el test de Wald para comprobar si existe dependencia espacial en los residuos del modelo. Los valores significativos (\< 0.05) en estos tests indica que aún existe una cierta autocorrelación que queda a pesar de usar el modelo de error espacial. En definitiva, este modelo ha mejorado en cierta medida el ajuste (se puede ver en la comparación del AIC respecto al del modelo lineal), pero que no ha abordado del todo los problemas espaciales que hemos señalado antes.

### Spatial lag model

Ajustemos ahora un modelo de *lag* espacial. Este modelo sí asume que los valores de y presentan una estructura espacial real, de manera que los valores de $y_i$ dependerán de los valores de $y$ en los alrededores de $i$.

![Fig.2 Estructura de un modelo de lag espacial](images/spatial_lag.png){fig-align="center"}

Para ello, un modelo de *lag* espacial añade una variable independiente adicional, que toma el valor de la media de los vecinos de cada observación. De esta manera, no tratamos la variable dependiente como espacialmente independiente en cada observación, sino que añadimos un término que asume explícitamente que las observaciones son parcialmente dependientes de los vecinos.

$y_i= \beta_0 + \beta_1X_1+\rho\omega_iy_i+\epsilon_i$

Un valor positivo de $\rho$ indica que el valor de $y_i$ aumentará cuando lo hagan los vecinos de $x_i$.Este parámetro $\rho$ no se mostrará en la salida de resultados "estándar", pero el modelo de spatial lag la incluye.

Usaremos ahora la función `lagsarlm()`, también del paquete `spatialreg`, y con la misma notación que antes:

```{r}
lag_model <- lagsarlm(formula =T_MAX_abs ~ ALTITUD, data = estaciones, listw = list_veins, zero.policy = T)
summary(lag_model)
```

Al incluir ese término de *lag* espacial, vemos que tanto el test de LR como el de Wald ya no detectan autocorrelación espacial. Sin mebargo, en este caso el modelo resulta muy similar al modelo lineal en términos de AIC.

### Selección del mejor modelo

Sin embargo, no siempre pasa esto. En numerosos casos, ni el modelo de *error espacial* ni el de *lag espacial* son capaces de eliminar la autocorrelación del todo, a pesar de reducirla. En otros casos, al contrario, ambos modelos son capaces de considerar y eliminar la autocorrelación.

Para elegir cuál es el mejor de los dos modelos, podemos usar, además de la comparación de los valores de AIC, y de la comprobación de si eliminan la autocorrelación espacial de los residuos, el test del multiplicador de Lagrange. Este valor nos indicará si una unidad dada (una estación meteorológica) está aún influenciada por las estaciones a su alrededor incluso después de tener en cuenta y corregir la autocorrelación espacial.En lineas generales, el modelo que presenta un valor mayor del test del Multiplicador de Lagrange es el modelo más adecuado.

Para ejecutar el test del multiplicador de Lagrange usaremos la función `lm.LMtests()`, del paquete `spatialreg`. A esta función le debemos proporcionar como argumentos el modelo original (el que no tiene en cuenta la estructura espacial), el nombre del listado de vecinos, y los tipos de modelos espaciales que queremos testar (en este caso `LMerr` para el modelo de error espacial, y `LMlag` para el lag espacial):

```{r}
test_Lagrange <- lm.LMtests(model = mod_lm,
                            listw = list_veins, 
                            test=c("LMerr", "LMlag"))

test_Lagrange
summary(test_Lagrange)

```

En este caso se confirma lo que habíamos visto antes: a pesar de que el modelo de error espacial no soluciona del todo el problema de la autocorrelación, resulta un mejor modelo, y es preferible respecto al modelo de lag espacial.

### Pasos a seguir ante la posibilidad de aplicar una regresión espacial

En resumen, si queremos realizar una regresión con un set de datos en el que sospechamos que podemos tener autocorrelación espacial, estos son los principales pasos a seguir:

1.  Como siempre, el primer paso es visualizar los datos (en este caso espacialmente), para ver si hay algún outlier o valores que falten, y para intuir la estructura espacial

2.  Si se intuye estructura espacial, debemos construir un vecindario espacial apropiado y asignar pesos

3.  Ajustar una regresión lineal (no espacial), que asume independencia de los residuos

4.  Inspeccionar los residuos, y calcular su valor de I de Moran

5.  Si observamos signos de dependencia espacial, ajustar modelos de error espacial y/o de lag espacial, y evaluar su idoneidad mediante el estadístico del multiplicador de Lagrange

6.  Para ser realmente minuciosos, comprobar la sensibilidad de los principales resultados de las regresiones a diferentes supuestos de vecindad/ponderación.

## Geographically weighted regression

Hemos visto que una tercera opción para tener en cuenta la estructura espacial de los datos es realizar una regresión ponderada geográficamente (geographically weighted regression). GWR es el término introducido por Fotheringham, Charlton y Brunsdon (1997, 2002) para describir una familia de modelos de regresión en los que se permite que los coeficientes varíen espacialmente. GWR funciona moviendo una ventana de búsqueda (kernel) de un punto de la muestra al siguiente, trabajando secuencialmente a través de todos los puntos existentes en el conjunto de datos. A continuación, se ajusta un modelo de regresión a todos los datos contenidos en la ventana identificada alrededor de cada punto, ponderando más los puntos de datos más cercanos al punto de muestra que los más alejados. Este proceso se repite para todos los puntos del conjunto de datos.

Para un conjunto de datos de 150 observaciones, GWR ajustará 150 modelos de regresión ponderados. La diferencia de este método respecto a los anteriores es que no asume que la relación entre *y* y *x* es constante en toda la zona de estudio, e incluso permite que haya cambios de signo locales en la relación entre ambas. Por todo ello se suele considerar el método más flexible.

### Kernel fijo o variable

Una cuestión clave es decidir entre dos opciones de kernels espaciales: un kernel fijo o un kernel adaptativo o variable. Intuitivamente, un **kernel fijo** implica el uso de un ancho de banda fijo para definir una región alrededor de cada uno los puntos de regresión, como se muestra en la Figura de abajo. La extensión del kernel viene determinada por la distancia al punto para el que se quiere ajustar la regresión, siendo el kernel idéntico en cualquier punto del espacio. Un **kernel adaptativo** implica el uso de un ancho de banda variable para definir una región alrededor de los puntos de regresión, tal y como se muestra abajo. La extensión del kernel viene determinada por el número de vecinos más próximos de un punto de regresión dado. Los kernel variables tienen anchos de banda mayores cuando los datos son escasos.

![Fig.3 Ejemplo de kernel con amplitud de banda fija](https://gdsl-ul.github.io/san/figs/ch8/fixed_bandwidth.png)

![Fig.4 Ejemplo de kernel con amplitud de banda variable](https://gdsl-ul.github.io/san/figs/ch8/fixed_bandwidth.png)

### Optimizando la amplitud de banda del kernel

El primer paso para ajustar una GWR es determinar el ancho de banda que se utilizará para seleccionar las sucesivas muestras. Esto lo haremos con la función `gwr.sel()` del paquete `spgwr`. Esta función ncesita que le especifiquemos la fórmula del modelo a ajustar y el dataset, pero además tiene un argumento más (`adapt`) para definir el tipo de ancho de banda a usar para definir la muestra de cada modelo de regresión parcial que ajustemos. Dicho ancho de banda puede ser fijo (`adapt = FALSE`) o variable (`adapt = TRUE`).

#### GWR con ancho de banda fijo

Si ajustamos un ancho de banda fijo el resultado de la función `gwr.sel` es una distancia de muestreo. Lo que hace la función es ajustar modelos con diferentes anchos de banda y compara los residuos por validación cruzada. El resultado será la distancia que minimiza los residuos.

```{r}

# Ancho de banda fijo
kernel_fijo <- gwr.sel(T_MAX_abs ~ ALTITUD, data=estaciones, coords = st_coordinates(estaciones), adapt=FALSE) 
kernel_fijo
```

En este caso vemos que el ancho previsto son `r kernel_fijo` metros.

Podemos ahora ajustar el modelo GWR mediante la función `gwr`, en la que el ancho de banda óptimo anterior se utiliza como entrada en el argumento `bandwidth`.

```{r}
gwr.model = gwr(T_MAX_abs ~ ALTITUD, data=estaciones, coords = st_coordinates(estaciones), 
                bandwidth = kernel_fijo, 
                hatmatrix = TRUE, se.fit=TRUE) 
```

Vemos que ahora no tenemos un sólo valor de los parámetros del modelo, como habitualmente, sino una serie de valores, de los que el otuput nos informa del mínimo, cuartiles y máximo. Los valores globales de los coeficientes coincidirán con los del modelo de regresión no espacial. Después veremos como interpretar algunos de estos resultados, de momento vamos a tomar los coeficientes, que se guardan en un `spatialDataFrame` (SDF) dentro del objeto del modelo. Este SDF contiene los coeficientes del modelo (que se guardan con el nombre de la variable a que multiplican), el error estandard de los coeficientes, las predicciones, y los valores de R2 de cada uno de los `n` modelos ajustados:

```{r}
View(gwr.model$SDF@data)
```

Podemos guardar los coeficientes estimados con nuestro dataset original (`estaciones`), para así poder plotear los valores:

```{r}
estaciones_gwr <- cbind(estaciones, gwr.model$SDF@data)
```

Podemos por tanto visualizar los coeficientes estimados por el modelo y ver cómo varían espacialmente:

```{r}
tmap_mode("view")
tm_shape(estaciones_gwr) +
    tm_dots(col="ALTITUD.1")
```

Vemos que, si bien la mayoría de los valores del coeficiente entre altitud y temperatura son negativos (lo que cabría esperar), algunos son incluso positivos. En concreto, los de la zona más norte del valle del Ebro, en la provincia de Zaragoza. Podemos plotear también los valores locales de R2

```{r}
tmap_mode("view")
tm_shape(estaciones_gwr) +
    tm_dots(col="localR2", palette = "viridis", style = "cont")
```

Vemos que los valores de R2 son razonablemente altos para la mayoría de los puntos, salvo en una zona del norte del Ebro (no la misma que tenía valores positivos de los coeficientes). En los GWR de ancho de banda fijo, esto pasa a veces en zonas con menor densidad de puntos, donde el modelo se ajusta con menor muestra. Veamos si cambian los resultados con un modelo de ancho de banda variable:

#### GWR con ancho de banda variable

Los anchos de banda variables adaptan su tamaño a la densidad de puntos existente, de manera que se hacen más amplios en las zonas donde hay menos muestra y más estrechos en zonas con más densidad de puntos. Como antes, el primer paso es calcular el valor óptimo:

```{r}
kernel_variable <- gwr.sel(T_MAX_abs ~ ALTITUD, data=estaciones, 
                           coords = st_coordinates(estaciones),
                           adapt=TRUE) 
kernel_variable
```

Esto quiere decir que el kernel óptimo es el que incluye un 2.8% (3%) de las observaciones vecinas en cada punto. Parece que con considerar los 3 vecinos más próximos a cada estación es suficiente para tener un modelo adecuado. Podemos ahora ajsutar el modelo de ancho variable, con la diferencia que ahora debemos usar el kernel que acabamos de estimar dentro del argumento `adapt`:

```{r}
gwr.model_var = gwr(T_MAX_abs ~ ALTITUD, data=estaciones, coords = st_coordinates(estaciones), 
                    adapt =kernel_variable, 
                    hatmatrix = TRUE, se.fit=TRUE) 
gwr.model_var
```

Podemos guardar los coeficientes estimados con nuestro dataset original (`estaciones`), para así poder plotear los valores:

```{r}
estaciones_gwr_var <- cbind(estaciones, gwr.model_var$SDF@data)
```

Volvamos a visualizar la distribución espacial de los valores estimados, coeficientes y R2 como antes:

```{r}
tmap_mode("view")
tm_shape(estaciones_gwr_var) +
    tm_dots(col="ALTITUD.1")
```

Quizá el contraste entre los valores negativos y positivos de los coeficientes sea incluso más marcado. Veamos si se ha mejorado la estimación de R2 de los puntos donde teníamos valores bajos:

```{r}
tmap_mode("view")
tm_shape(estaciones_gwr_var) +
    tm_dots(col="localR2", palette = "viridis", style = "cont")
```

Parece que no, por lo que el bajo R2 no se debería a una escasez de datos, sino a que en aquella zona la relación entre altitud y temperatura es más débil. Esto puede tener sentido en zonas llanas junto a los ríos, donde incluso es común tener fenómenos de inversión térmica que trastocan la relación negativa entre ambas variables, la más habitual y esperada.
